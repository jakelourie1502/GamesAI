{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen Lake\n",
    "\n",
    "Frozen lake is a game, where a bot has to get from one corner of the lake to another without falling down a hole.\n",
    "\n",
    "The bot \"decides\" which way to go, but there is also randomness in this - i.e. the computer sometimes makes a different decision for us\n",
    "\n",
    "We need to train the bot to make the right decisions\n",
    "\n",
    "Here is what the 'board' looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "This is a deterministic game. Under the hood, we can see the probabilities associated with any 'move' at a certain square. For example, here, in state 10\n",
    "\n",
    "- if we press 0(left), we have a 1/3 chance of going left, a 1/3 chance of going up, and a 1/3 chance of going right.\n",
    "\n",
    "- The second column refers to the step we would be in\n",
    "\n",
    "- the third column refers to the reward associated with this move\n",
    "\n",
    "- the fourth column refers to whether this would end the game\n",
    "\n",
    "- Each 'direction' only has three options in this game, so for one of the directions, the chance is 0. For example, if we press 0 (left), we cannot reach square 7(right, in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 2, 0.0, False),\n",
       "  (0.3333333333333333, 5, 0.0, True),\n",
       "  (0.3333333333333333, 10, 0.0, False)],\n",
       " 1: [(0.3333333333333333, 5, 0.0, True),\n",
       "  (0.3333333333333333, 10, 0.0, False),\n",
       "  (0.3333333333333333, 7, 0.0, True)],\n",
       " 2: [(0.3333333333333333, 10, 0.0, False),\n",
       "  (0.3333333333333333, 7, 0.0, True),\n",
       "  (0.3333333333333333, 2, 0.0, False)],\n",
       " 3: [(0.3333333333333333, 7, 0.0, True),\n",
       "  (0.3333333333333333, 2, 0.0, False),\n",
       "  (0.3333333333333333, 5, 0.0, True)]}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=3,suppress=True)\n",
    "env.P[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Treating it as a markov decision problem\n",
    "---\n",
    "Here we will try two approaches.\n",
    "\n",
    "1) Policy evaluation - approach this as incrementing on a chosen policy until the policy doesn't change\n",
    "\n",
    "2) Action value iteration - approach as converging on a value for each state, and using the deterministic probabilities which move is the best to make\n",
    "\n",
    "Here we start with all the states being zero, and then iteratively updating the value of each square, and then using the value fucntion, which incorporates the probability of being in a future state and that state's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
    "    # Create a counter which we return later to show how long it took\n",
    "    evaluation_iterations = 1\n",
    "    # Initialize a value function for each state as zero\n",
    "    V = np.zeros(environment.nS) #V is value matrix, v is value of a state\n",
    "    \n",
    "    '''goal is to keep iterating until the change in value is very very small (below a threshold)'''\n",
    "\n",
    "    for i in range(int(max_iterations)):\n",
    "        # Initialize a change of value function as zero\n",
    "        delta = 0\n",
    "        # Iterate though each state\n",
    "        for state in range(environment.nS):\n",
    "           # Initialize a new value of current state\n",
    "            v = 0\n",
    "            '''For each action and its probability in a given state'''\n",
    "            for action, action_probability in enumerate(policy[state]):\n",
    "                '''look at the possible next states of these actions, the prob of that, the reward of that whether it lead to termination'''\n",
    "                for state_probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "                    # '''calculate expected value which is the chance of this value happening * reward and the value in the matrix currently of the next state'''\n",
    "                    v += action_probability * state_probability * (reward + discount_factor * V[next_state]) #on iteration one, only current reward is considered\n",
    "\n",
    "           # Calculate the absolute change of value function\n",
    "            delta = max(delta, np.abs(V[state] - v)) #at beginning of iteration thorugh the whole thing, delta set to 0. so this will capture the highest state delta\n",
    "           # Update value function\n",
    "            V[state] = v\n",
    "        evaluation_iterations += 1\n",
    "\n",
    "        # Terminate if value change is insignificant\n",
    "        if delta < theta:\n",
    "            print(f'Policy evaluated in {evaluation_iterations} iterations.')\n",
    "            return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_lookahead(environment, state, V, discount_factor):\n",
    "    '''this is a function fo a given state, what is the value of each action, so returns a vector of 4 values (if up down left right)'''\n",
    "    action_values = np.zeros(environment.nA) #nA = (4,1)\n",
    "    for action in range(environment.nA):\n",
    "        for probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "            action_values[action] += probability * (reward + discount_factor * V[next_state])\n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(environment, discount_factor=1.0, max_iterations=1e9):\n",
    "    # Start with a random policy\n",
    "    #this is a state x action matrix\n",
    "    policy = np.ones([environment.nS, environment.nA]) / environment.nA\n",
    "    # Initialize counter of evaluated policies\n",
    "    evaluated_policies = 1\n",
    "    # Repeat until convergence or critical number of iterations reached\n",
    "    for i in range(int(max_iterations)):\n",
    "        stable_policy = True\n",
    "        # Evaluate current policy = create a V matrix of value of any state.\n",
    "        V = policy_evaluation(policy, environment, discount_factor=discount_factor)\n",
    "        # Go through each state and try to improve actions that were taken (policy Improvement)\n",
    "        for state in range(environment.nS):\n",
    "            # '''Pick the action in this state which currently has the highest probability of occuring, or \"current action\" '''\n",
    "            current_action = np.argmax(policy[state])\n",
    "            # Look one step ahead and evaluate if current action is optimal\n",
    "            # We will try every possible action in a current state\n",
    "            action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "            # Select a better action\n",
    "            best_action = np.argmax(action_value)\n",
    "            # If action didn't change\n",
    "            if current_action != best_action:\n",
    "                stable_policy = False\n",
    "                # Greedy policy update\n",
    "                policy[state] = np.eye(environment.nA)[best_action] #creates a policy of one action per state.\n",
    "        evaluated_policies += 1\n",
    "        # If the algorithm converged and policy is not changing anymore, then return final policy and value function\n",
    "        if stable_policy:\n",
    "            print(f'Evaluated {evaluated_policies} policies.')\n",
    "            return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
    "    # Initialize state-value function with zeros for each environment state\n",
    "    V = np.zeros(environment.nS)\n",
    "    for i in range(int(max_iterations)):\n",
    "        # Early stopping condition\n",
    "        delta = 0\n",
    "        # Update each state\n",
    "        for state in range(environment.nS):\n",
    "            # Do a one-step lookahead to calculate state-action values\n",
    "            action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "            # Select best action to perform based on the highest state-action value\n",
    "            best_action_value = np.max(action_value)\n",
    "            # Calculate change in value\n",
    "            delta = max(delta, np.abs(V[state] - best_action_value))\n",
    "            # Update the value function for current state\n",
    "            V[state] = best_action_value\n",
    "            # Check if we can stop\n",
    "        if delta < theta:\n",
    "            print(f'Value-iteration converged at iteration#{i}.')\n",
    "            break\n",
    "\n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros([environment.nS, environment.nA])\n",
    "    for state in range(environment.nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "        # Select best action based on the highest state-action value\n",
    "        best_action = np.argmax(action_value)\n",
    "        # Update the policy to perform a better action at a current state\n",
    "        policy[state, best_action] = 1.0\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at iteration#523.\n",
      "Value Iteration :: number of wins over 10000 episodes = 7402\n",
      "Value Iteration :: average reward over 10000 episodes = 0.7402 \n",
      "\n",
      "\n",
      "Policy evaluated in 66 iterations.\n",
      "Policy evaluated in 170 iterations.\n",
      "Policy evaluated in 428 iterations.\n",
      "Evaluated 4 policies.\n",
      "Policy Iteration :: number of wins over 10000 episodes = 7387\n",
      "Policy Iteration :: average reward over 10000 episodes = 0.7387 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def play_episodes(environment, n_episodes, policy):\n",
    "    wins = 0\n",
    "    total_reward = 0\n",
    "    for episode in range(n_episodes):\n",
    "        terminated = False\n",
    "        state = environment.reset()\n",
    "        while not terminated:\n",
    "            # Select best action to perform in a current state\n",
    "            action = np.argmax(policy[state])\n",
    "            # Perform an action an observe how environment acted in response\n",
    "            next_state, reward, terminated, info = environment.step(action)\n",
    "            # Summarize total reward\n",
    "            total_reward += reward\n",
    "            # Update current state\n",
    "            state = next_state\n",
    "            # Calculate number of wins over episodes\n",
    "            if terminated and reward == 1.0:\n",
    "                wins += 1\n",
    "    average_reward = total_reward / n_episodes\n",
    "    return wins, total_reward, average_reward\n",
    "\n",
    "# Number of episodes to play\n",
    "n_episodes = 10000\n",
    "# Functions to find best policy\n",
    "solvers = [('Value Iteration', value_iteration),('Policy Iteration', policy_iteration)]\n",
    "           \n",
    "for iteration_name, iteration_func in solvers:\n",
    "    # Load a Frozen Lake environment\n",
    "    environment = gym.make('FrozenLake-v0')\n",
    "    # Search for an optimal policy using policy iteration\n",
    "    policy, V = iteration_func(environment.env)\n",
    "    # Apply best policy to the real environment\n",
    "    wins, total_reward, average_reward = play_episodes(environment, n_episodes, policy)\n",
    "    print(f'{iteration_name} :: number of wins over {n_episodes} episodes = {wins}')\n",
    "    print(f'{iteration_name} :: average reward over {n_episodes} episodes = {average_reward} \\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Treating it as a Monte Carlo problem\n",
    "---\n",
    "Here we randomly pick a state, play a game with a policy and calculate the return from a given S/A policy. We need some way of not always making the same decision just because it once gave us a reward, so our next decision on what to do is a mix between max(S/A) and a small random epsilon value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_dict = {0:'left',1:'down',2:'right',3:'up'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "8 1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "12 0\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n"
     ]
    }
   ],
   "source": [
    "#An example of playing the game\n",
    "env.reset()\n",
    "done = False\n",
    "while done == False:\n",
    "    action = np.random.choice(array_of_choices,p=random_policy_matrix[state])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(state,action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_of_choices = np.array([0,1,2,3])\n",
    "\n",
    "random_policy_matrix = np.zeros((16,4)) + 0.25\n",
    "\n",
    "'''function to initialise the state'''\n",
    "def init_env():\n",
    "    sa_occurence_matrix = np.zeros((16,4)) #needs to be reinitialized every time.\n",
    "    env.reset()\n",
    "    done = False\n",
    "    state=0\n",
    "    rew= 0\n",
    "    return sa_occurence_matrix, env, done, state, rew\n",
    "\n",
    "def one_iteration(sa_valmat,policy='random',epsilon=0.1):\n",
    "    '''sa_valmat is a 16x4x2 matrix, (16 states, 4 actions, and a channel for count and value)'''\n",
    "    sa_occurence_matrix, env, done, state, rew = init_env()\n",
    "    step_count = 0\n",
    "    while done == False:\n",
    "        step_count +=1\n",
    "        if policy == 'random':\n",
    "            action = np.random.choice(array_of_choices,p=random_policy_matrix[state])\n",
    "        elif policy == 'tiny_epsilon':\n",
    "            action = tiny_epsilon_choice(sa_valmat,state, random_policy_matrix,epsilon = epsilon)\n",
    "        elif policy == 'softmax':\n",
    "            action = np.random.choice(array_of_choices,p=np.exp(sa_valmat[state,:,0])**0.3/np.sum(np.exp(sa_valmat[state,:,0])**0.3))\n",
    "        sa_occurence_matrix[state,action] += 1 #could be plus equal one if we're using each time.\n",
    "        state, reward, done, info = env.step(action)\n",
    "        rew = max(rew,reward)\n",
    "    for i in range(sa_valmat.shape[0]):\n",
    "        for j in range(sa_valmat.shape[1]):\n",
    "            if sa_occurence_matrix[i,j] >= 1:\n",
    "                #increment the count channel\n",
    "                sa_valmat[i,j,1] += sa_occurence_matrix[i,j]\n",
    "                #get new value average column - here we allocate a point every time we took an action in a state - so if we went left three times from 4, that would be incremented by 4.\n",
    "                sa_valmat[i,j,0] += (sa_occurence_matrix[i,j]/ sa_valmat[i,j,1]) * (rew - sa_valmat[i,j,0])\n",
    "    \n",
    "    return sa_valmat, rew, step_count\n",
    "\n",
    "\n",
    "def tiny_epsilon_choice(sa_valmat, state, random_policy_matrix, epsilon,rubber='no'):\n",
    "    '''select the best option most of the time'''\n",
    "    random_shot = np.random.uniform(0,1)\n",
    "    if random_shot >= epsilon:\n",
    "        action = np.argmax(sa_valmat[state,:,0])\n",
    "    else:\n",
    "        action = np.random.choice(array_of_choices,p=random_policy_matrix[state])\n",
    "    return action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's train and test this for the three different epsilon policies\n",
    "\n",
    "- Random: allowing it to at randomly pick a route, max exploration\n",
    "\n",
    "- Softmax - using a softmax function with a dampener of 0.3 (so it's slightly more exploratory)\n",
    "\n",
    "- Using Tiny Epsilon, with a value of 0.2 to pick the random choice sometimes and the best choice most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(policy,train_epochs,test_epochs):\n",
    "    #create blank state action matrix with 2 channels (one for value, one for N(sa))\n",
    "    sa_valmat = np.zeros((16,4,2)) #we'll update it using v(sa) = v(sa) 1/N(sa) + [r(t) - v(sa)]\n",
    "    sa_valmat[:,:,0] +=0.01 #create tiny bit of value for all SA's\n",
    "    random_policy_matrix = np.zeros((16,4)) + 0.25\n",
    "    array_of_choices = np.array([0,1,2,3])\n",
    "    \n",
    "    #we're going to initialise the game 2000 times, and randomly walk throughout the game and then assign an award to each s/a that occurs.\n",
    "    '''2000 random ones to start with'''\n",
    "    reward_log =0\n",
    "    for i in range(10):\n",
    "        sa_valmat, rew, _ = one_iteration(sa_valmat,policy='random')\n",
    "        reward_log += rew\n",
    "    \n",
    "    logging_train_step_count = 0\n",
    "    for i in range(train_epochs):\n",
    "        #used when we need it, converges to about 0.05 after high number of trials\n",
    "        epsilon = 0.5 - (np.exp(i/train_epochs)/(1+np.exp(i/train_epochs)))/2\n",
    "        sa_valmat, rew, step_count = one_iteration(sa_valmat,policy=policy,epsilon=epsilon)\n",
    "        reward_log+=rew\n",
    "        logging_train_step_count += step_count\n",
    "    print(logging_train_step_count)\n",
    "    # test policy\n",
    "    logging_test_step_count = 0\n",
    "    wins = 0\n",
    "    rang = 2000\n",
    "    for i in range(test_epochs):\n",
    "        epsilon = 0\n",
    "        sa_valmat, rew_pol, step_count = one_iteration(sa_valmat,policy='tiny_epsilon',epsilon=epsilon) #here we use tiny epsilon with epsilon equals 0\n",
    "        wins += rew_pol\n",
    "        logging_test_step_count+= step_count\n",
    "        \n",
    "    return wins, wins/test_epochs, reward_log, reward_log/train_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3835497\n",
      "9004453\n",
      "3838047\n"
     ]
    }
   ],
   "source": [
    "softmax_figures = train_and_test('softmax', 500000, 5000)\n",
    "tiny_ep_figures = train_and_test('tiny_epsilon', 500000, 5000)\n",
    "random_pol_figures = train_and_test('random', 500000, 5000)\n",
    "\n",
    "\n",
    "# SM_test_wins, SM_test_acc, SM_train_wins, SM_train_acc = train_and_test('softmax', 500000, 2000)\n",
    "# TE_test_wins, TE_test_acc, TE_train_wins, TE_train_acc = train_and_test('tiny_epsilon', 500000, 2000)\n",
    "# RP_test_wins, RP_test_acc, RP_train_wins, RP_train_acc = train_and_test('random', 500000, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>softmax</th>\n",
       "      <th>tiny epsilon</th>\n",
       "      <th>random policy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Test wins</th>\n",
       "      <td>3613.000</td>\n",
       "      <td>2499.000</td>\n",
       "      <td>3645.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test acc</th>\n",
       "      <td>0.723</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train wins</th>\n",
       "      <td>7199.000</td>\n",
       "      <td>100993.000</td>\n",
       "      <td>6948.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train acc</th>\n",
       "      <td>0.014</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             softmax  tiny epsilon  random policy\n",
       "Test wins   3613.000      2499.000       3645.000\n",
       "test acc       0.723         0.500          0.729\n",
       "train wins  7199.000    100993.000       6948.000\n",
       "train acc      0.014         0.202          0.014"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option(\"display.precision\", 3)\n",
    "summary_table = pd.DataFrame(data=[softmax_figures,tiny_ep_figures,random_pol_figures],columns=['Test wins','test acc','train wins','train acc']).T.rename(columns={0:'softmax',1:'tiny epsilon',2:'random policy'})\n",
    "display(summary_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "Softmax and Random policy had a lower train accuracy, as they tend to favour exploration over exploitation.\n",
    "\n",
    "tiny epsilon recorded significantly higher train wins; however, due to its favouring of what it believed to be the best solutions, it failed to explore enough, and there doesn't have the accuracy in the test that the other algos created.\n",
    "\n",
    "finally, it's clear that we achieved the best results using a deterministic approach, which makes sense, because if this approach is available to us, it will normally yield the best answer, quickest.\n",
    "\n",
    "However, we have shown that using a non-deterministic approach, we can achieve the same results as the Markov approach. This will bode well if we need to apply it to a non-deterministic problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "ae44bc0eae4a697e13956773fab0a10687b55ffe583901dc88e01866ea6f570f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
