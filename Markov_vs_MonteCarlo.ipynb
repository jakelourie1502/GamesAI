{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "env = gym.make('FrozenLake-v0')\n",
    "pd.set_option(\"display.precision\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing performance of a Markov process to non-deterministic Monte Carlo solver for a deterministic game\n",
    "\n",
    "For simple games, where the outcome probabilities of each move are known, and where the game is not so large that computing power makes a markov process impractical, the Markov method should yield the best outcome when training an agent. In this notebook, though, my motivation is to see if using a Monte Carlo process can yield similarly powerful results, so we can understand the potential power of the Monte Carlo method when applied to more complex, non-deterministic problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### The Game - Frozen Lake\n",
    "\n",
    "Frozen lake is a game, where a bot has to get from one corner of the lake to another without falling down a hole. https://gym.openai.com/envs/FrozenLake-v0/\n",
    "\n",
    "The bot \"decides\" which way to go, but there is also randomness in this - i.e. the computer sometimes makes a different decision for us\n",
    "\n",
    "We need to train the bot to make the right decisions. Here is what the 'board' looks like:\n",
    "\n",
    "S = Start\n",
    "\n",
    "F = Frozen (can walk on)\n",
    "\n",
    "H = Hole (game ends)\n",
    "\n",
    "G = Goal (game ends and you get a reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "This is a deterministic game. Under the hood, we can see the probabilities associated with any 'move' at a certain square. For example, here, in state 10\n",
    "\n",
    "- if we press 0(left), we have a 1/3 chance of going left, a 1/3 chance of going up, and a 1/3 chance of going right.\n",
    "\n",
    "- The second column refers to the step we would be in\n",
    "\n",
    "- the third column refers to the reward associated with this move\n",
    "\n",
    "- the fourth column refers to whether this would end the game\n",
    "\n",
    "- Each 'direction' only has three options in this game, so for one of the directions, the chance is 0. For example, if we press 0 (left), we cannot reach square 7(right, in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 2, 0.0, False),\n",
       "  (0.3333333333333333, 5, 0.0, True),\n",
       "  (0.3333333333333333, 10, 0.0, False)],\n",
       " 1: [(0.3333333333333333, 5, 0.0, True),\n",
       "  (0.3333333333333333, 10, 0.0, False),\n",
       "  (0.3333333333333333, 7, 0.0, True)],\n",
       " 2: [(0.3333333333333333, 10, 0.0, False),\n",
       "  (0.3333333333333333, 7, 0.0, True),\n",
       "  (0.3333333333333333, 2, 0.0, False)],\n",
       " 3: [(0.3333333333333333, 7, 0.0, True),\n",
       "  (0.3333333333333333, 2, 0.0, False),\n",
       "  (0.3333333333333333, 5, 0.0, True)]}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Treating it as a markov decision problem\n",
    "\n",
    "For deterministic games, a Markov decision process will work, assuming there are not too many combinations and state-action pairs for the computer to handle. This is quite a simple game, so there won't be.\n",
    "\n",
    "Markov generally works by calculating the 'value' of any state action by making it equal to the immediate reward + the value of the state it leads to you to move to. By recursively calculating this for all states, we gradually converge to a stable state-action value matrix.\n",
    "\n",
    "We will approach this is a policy model, which has three 'steps':\n",
    "\n",
    "1) Define a function to evaluate the value of each state according to a certain policy\n",
    "\n",
    "2) for a given state, determine the value of each action\n",
    "\n",
    "3) iterate on the policy, by changing the 'best' action in the policy if another action has a higher value.\n",
    "\n",
    "Here we start with all the states being zero, and then iteratively updating the value of each square, and then using the value fucntion, which incorporates the probability of being in a future state and that state's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
    "    # Create a counter which we return later to show how long it took\n",
    "    evaluation_iterations = 1\n",
    "    # Initialize a value function for each state as zero\n",
    "    V = np.zeros(environment.nS) #V is value matrix, v is value of a state\n",
    "    \n",
    "    '''goal is to keep iterating until the change in value is very very small (below a threshold)'''\n",
    "\n",
    "    for i in range(int(max_iterations)):\n",
    "        # Initialize a change of value function as zero\n",
    "        delta = 0\n",
    "        # Iterate though each state\n",
    "        for state in range(environment.nS):\n",
    "           # Initialize a new value of current state\n",
    "            v = 0\n",
    "            '''For each action and its probability in a given state'''\n",
    "            for action, action_probability in enumerate(policy[state]):\n",
    "                '''look at the possible next states of these actions, the prob of that, the reward of that whether it lead to termination'''\n",
    "                for state_probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "                    # '''calculate expected value which is the chance of this value happening * reward and the value in the matrix currently of the next state'''\n",
    "                    v += action_probability * state_probability * (reward + discount_factor * V[next_state]) #on iteration one, only current reward is considered\n",
    "\n",
    "           # Calculate the absolute change of value function\n",
    "            delta = max(delta, np.abs(V[state] - v)) #at beginning of iteration thorugh the whole thing, delta set to 0. so this will capture the highest state delta\n",
    "           # Update value function\n",
    "            V[state] = v\n",
    "        evaluation_iterations += 1\n",
    "\n",
    "        # Terminate if value change is insignificant\n",
    "        if delta < theta:\n",
    "            print(f'Policy evaluated in {evaluation_iterations} iterations.')\n",
    "            return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_lookahead(environment, state, V, discount_factor):\n",
    "    '''this is a function fo a given state, what is the value of each action, so returns a vector of 4 values (if up down left right)'''\n",
    "    action_values = np.zeros(environment.nA) #nA = (4,1)\n",
    "    for action in range(environment.nA):\n",
    "        for probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "            action_values[action] += probability * (reward + discount_factor * V[next_state])\n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(environment, discount_factor=1.0, max_iterations=1e9):\n",
    "    # Start with a random policy\n",
    "    #this is a state x action matrix\n",
    "    policy = np.ones([environment.nS, environment.nA]) / environment.nA\n",
    "    # Initialize counter of evaluated policies\n",
    "    evaluated_policies = 1\n",
    "    # Repeat until convergence or critical number of iterations reached\n",
    "    for i in range(int(max_iterations)):\n",
    "        stable_policy = True\n",
    "        # Evaluate current policy = create a V matrix of value of any state.\n",
    "        V = policy_evaluation(policy, environment, discount_factor=discount_factor)\n",
    "        # Go through each state and try to improve actions that were taken (policy Improvement)\n",
    "        for state in range(environment.nS):\n",
    "            # '''Pick the action in this state which currently has the highest probability of occuring, or \"current action\" '''\n",
    "            current_action = np.argmax(policy[state])\n",
    "            # Look one step ahead and evaluate if current action is optimal\n",
    "            # We will try every possible action in a current state\n",
    "            action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "            # Select a better action\n",
    "            best_action = np.argmax(action_value)\n",
    "            # If action didn't change\n",
    "            if current_action != best_action:\n",
    "                stable_policy = False\n",
    "                # Greedy policy update\n",
    "                policy[state] = np.eye(environment.nA)[best_action] #creates a policy of one action per state.\n",
    "        evaluated_policies += 1\n",
    "        # If the algorithm converged and policy is not changing anymore, then return final policy and value function\n",
    "        if stable_policy:\n",
    "            print(f'Evaluated {evaluated_policies} policies.')\n",
    "            return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluated in 66 iterations.\n",
      "Policy evaluated in 170 iterations.\n",
      "Policy evaluated in 428 iterations.\n",
      "Evaluated 4 policies.\n",
      "Policy Iteration :: number of wins over 10000 episodes = 7386\n",
      "Policy Iteration :: average reward over 10000 episodes = 0.7386 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def play_episodes(environment, n_episodes, policy):\n",
    "    wins = 0\n",
    "    total_reward = 0\n",
    "    for episode in range(n_episodes):\n",
    "        terminated = False\n",
    "        state = environment.reset()\n",
    "        while not terminated:\n",
    "            # Select best action to perform in a current state\n",
    "            action = np.argmax(policy[state])\n",
    "            # Perform an action an observe how environment acted in response\n",
    "            next_state, reward, terminated, info = environment.step(action)\n",
    "            # Summarize total reward\n",
    "            total_reward += reward\n",
    "            # Update current state\n",
    "            state = next_state\n",
    "            # Calculate number of wins over episodes\n",
    "            if terminated and reward == 1.0:\n",
    "                wins += 1\n",
    "    average_reward = total_reward / n_episodes\n",
    "    return wins, total_reward, average_reward\n",
    "\n",
    "# Number of episodes to play\n",
    "n_episodes = 10000\n",
    "# Functions to find best policy\n",
    "solvers = [('Policy Iteration', policy_iteration)] #[('Value Iteration', value_iteration),\n",
    "           \n",
    "for iteration_name, iteration_func in solvers:\n",
    "    # Load a Frozen Lake environment\n",
    "    environment = gym.make('FrozenLake-v0')\n",
    "    # Search for an optimal policy using policy iteration\n",
    "    policy, V = iteration_func(environment.env)\n",
    "    # Apply best policy to the real environment\n",
    "    wins, total_reward, average_reward = play_episodes(environment, n_episodes, policy)\n",
    "    print(f'{iteration_name} :: number of wins over {n_episodes} episodes = {wins}')\n",
    "    print(f'{iteration_name} :: average reward over {n_episodes} episodes = {average_reward} \\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo Approach\n",
    "---\n",
    "Monte Carlo works by allowing the agent to 'explore', and then assigning any reward for the game just played to all the actions played in the game. For example, if we play a game and we get a reward of 1, and during the game we played \"left\" from square 2, then that state-action pair will be assigned a value of 1. Over time, actions that have been assigned high value in previous games will be played more frequently, although the rate at which we continue exploring low value options is a parameter which, when increased, can increase our chances of finding the optimal solution, but at the cost of poor performance in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: This is the dictionary of action indexes\n",
    "act_dict = {0:'left',1:'down',2:'right',3:'up'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Quick Example of playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "8 1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "12 0\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n"
     ]
    }
   ],
   "source": [
    "#An example of playing the game\n",
    "env.reset()\n",
    "done = False\n",
    "while done == False:\n",
    "    action = np.random.choice(array_of_choices,p=random_policy_matrix[state])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(state,action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Creating Core functions for the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''function to initialise the state'''\n",
    "\n",
    "def init_env():\n",
    "    sa_occurence_matrix = np.zeros((16,4)) #needs to be reinitialized every time.\n",
    "    env.reset()\n",
    "    done = False\n",
    "    state=0\n",
    "    rew= 0\n",
    "    return sa_occurence_matrix, env, done, state, rew\n",
    "\n",
    "'''Function that plays the game once, recording the states and actions and assigning reward as appropriate.'''\n",
    "\n",
    "def one_iteration(sa_valmat,policy='random',epsilon=0.1):\n",
    "    '''sa_valmat is a 16x4x2 matrix, (16 states, 4 actions, and a channel for count and value)'''\n",
    "    sa_occurence_matrix, env, done, state, rew = init_env()\n",
    "    step_count = 0\n",
    "    while done == False:\n",
    "        step_count +=1\n",
    "        if policy == 'random':\n",
    "            action = np.random.choice(array_of_choices,p=random_policy_matrix[state])\n",
    "        elif policy == 'tiny_epsilon':\n",
    "            action = tiny_epsilon_choice(sa_valmat,state, random_policy_matrix,epsilon = epsilon)\n",
    "        elif policy == 'softmax':\n",
    "            action = np.random.choice(array_of_choices,p=np.exp(sa_valmat[state,:,0])**0.3/np.sum(np.exp(sa_valmat[state,:,0])**0.3))\n",
    "        sa_occurence_matrix[state,action] += 1 #could be plus equal one if we're using each time.\n",
    "        state, reward, done, info = env.step(action)\n",
    "        rew = max(rew,reward)\n",
    "    for i in range(sa_valmat.shape[0]):\n",
    "        for j in range(sa_valmat.shape[1]):\n",
    "            if sa_occurence_matrix[i,j] >= 1:\n",
    "                #increment the count channel\n",
    "                sa_valmat[i,j,1] += sa_occurence_matrix[i,j]\n",
    "                #get new value average column - here we allocate a point every time we took an action in a state - so if we went left three times from 4, that would be incremented by 4.\n",
    "                sa_valmat[i,j,0] += (sa_occurence_matrix[i,j]/ sa_valmat[i,j,1]) * (rew - sa_valmat[i,j,0])\n",
    "    \n",
    "    return sa_valmat, rew, step_count\n",
    "\n",
    "'''Helper function for when we use tiny epsilon as an exploration method'''\n",
    "\n",
    "def tiny_epsilon_choice(sa_valmat, state, random_policy_matrix, epsilon,rubber='no'):\n",
    "    '''select the best option most of the time'''\n",
    "    random_shot = np.random.uniform(0,1)\n",
    "    if random_shot >= epsilon:\n",
    "        action = np.argmax(sa_valmat[state,:,0])\n",
    "    else:\n",
    "        action = np.random.choice(array_of_choices,p=random_policy_matrix[state])\n",
    "    return action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's train and test this for the three different epsilon policies\n",
    "\n",
    "- Random: allowing it to at randomly pick a route, max exploration\n",
    "\n",
    "- Softmax - using a softmax function with a dampener of 0.3 (so it's slightly more exploratory)\n",
    "\n",
    "- Using Tiny Epsilon, with a value of 0.2 to pick the random choice sometimes and the best choice most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(policy,train_epochs,test_epochs):\n",
    "    \n",
    "    '''Create empty state action value matrix, random policy matrix and the choices available to us'''\n",
    "    \n",
    "    sa_valmat = np.zeros((16,4,2)) #we'll update it using v(sa) = v(sa) 1/N(sa) + [r(t) - v(sa)]\n",
    "    sa_valmat[:,:,0] +=0.01 #create tiny bit of value for all SA's in order for divisors to work\n",
    "    random_policy_matrix = np.zeros((16,4)) + 0.25\n",
    "    array_of_choices = np.array([0,1,2,3])\n",
    "    \n",
    "    #We'll counter train and test wins over the course of the function\n",
    "    train_wins =0\n",
    "    test_wins = 0\n",
    "    \n",
    "    \n",
    "    '''we're going to initialise the game 2000 times, and randomly walk throughout the game and then assign an award to each s/a that occurs.'''     \n",
    "    for i in range(10):\n",
    "        sa_valmat, rew, _ = one_iteration(sa_valmat,policy='random')\n",
    "        train_wins += rew\n",
    "    \n",
    "    '''Then were going to train using tiny epsilon, random or softmax'''\n",
    "    for i in range(train_epochs):\n",
    "        #used when we need it, converges to about 0.05 after high number of trials\n",
    "        epsilon = 0.5 - (np.exp(i/train_epochs)/(1+np.exp(i/train_epochs)))/2\n",
    "        sa_valmat, rew, step_count = one_iteration(sa_valmat,policy=policy,epsilon=epsilon)\n",
    "        train_wins+=rew\n",
    "    \n",
    "    '''Then were going to test by setting epsilon to zero'''\n",
    "    for i in range(test_epochs):\n",
    "        epsilon = 0\n",
    "        sa_valmat, rew_pol, step_count = one_iteration(sa_valmat,policy='tiny_epsilon',epsilon=epsilon) #here we use tiny epsilon with epsilon equals 0\n",
    "        wins += rew_pol\n",
    "        logging_test_step_count+= step_count\n",
    "        \n",
    "    return test_win, test_wins/test_epochs, train_wins, train_wins/train_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3835497\n",
      "9004453\n",
      "3838047\n"
     ]
    }
   ],
   "source": [
    "softmax_figures = train_and_test('softmax', 500000, 5000)\n",
    "tiny_ep_figures = train_and_test('tiny_epsilon', 500000, 5000)\n",
    "random_pol_figures = train_and_test('random', 500000, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>softmax</th>\n",
       "      <th>tiny epsilon</th>\n",
       "      <th>random policy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Test wins</th>\n",
       "      <td>3613.000</td>\n",
       "      <td>2499.000</td>\n",
       "      <td>3645.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test acc</th>\n",
       "      <td>0.723</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train wins</th>\n",
       "      <td>7199.000</td>\n",
       "      <td>100993.000</td>\n",
       "      <td>6948.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train acc</th>\n",
       "      <td>0.014</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             softmax  tiny epsilon  random policy\n",
       "Test wins   3613.000      2499.000       3645.000\n",
       "test acc       0.723         0.500          0.729\n",
       "train wins  7199.000    100993.000       6948.000\n",
       "train acc      0.014         0.202          0.014"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary_table = pd.DataFrame(data=[softmax_figures,tiny_ep_figures,random_pol_figures],columns=['Test wins','test acc','train wins','train acc']).T.rename(columns={0:'softmax',1:'tiny epsilon',2:'random policy'})\n",
    "display(summary_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "Softmax and Random policy had a lower train accuracy, as they tend to favour exploration over exploitation.\n",
    "\n",
    "tiny epsilon recorded significantly higher train wins; however, due to its favouring of what it believed to be the best solutions, it failed to explore enough, and there doesn't have the accuracy in the test that the other algos created.\n",
    "\n",
    "finally, it's clear that we achieved the best results using a deterministic approach, which makes sense, because if this approach is available to us, it will normally yield the best answer, quickest.\n",
    "\n",
    "However, we have shown that using a non-deterministic approach, we can achieve the same results as the Markov approach. This will bode well if we need to apply it to a non-deterministic problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "ae44bc0eae4a697e13956773fab0a10687b55ffe583901dc88e01866ea6f570f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
